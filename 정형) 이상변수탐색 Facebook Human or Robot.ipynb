{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/facebook-recruiting-iv-human-or-bot/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/facebook-recruiting-iv-human-or-bot/test.csv.zip')\n# bids = pd.read_csv('/kaggle/input/facebook-recruiting-iv-human-or-bot/bids.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bids.head() # merge - 두 개 데이터 프레임 병합, 하나의 id에 대푯값(평균 등)을 넣어줌","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20:03\n# 힌트 -  인간은 일주일 동안 온라인에서 구매했을 시, 100번이상은 안넘어감 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bids['bidder_id'].value_counts() # 각각 id별로 구매를 몇번했는지 count를 추가해주면 모델의 점수가 높아질 것임","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bids.groupby('bidder_id')['bid_id'].count().reset_index() # 쉽게 df 만드는법\n# 행 6614 인데 몇개의 계정은 구매를 하지 않음 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bid_count = bids.groupby('bidder_id')['bid_id'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ip는 사람도 바뀌는 경우도 잇음\n#url은 점수에 도움이 안되는 데 그 이유는 다른 데이터셋에서 url의 정보를 담고 분석에 들어가서\n#time - 구매와 구매 사이의 구간이 짧으면 로봇일 확률\n# time  - 유행하는 시즌/집중 시기이 있을 것임 -> 연월일로 바꿔줘서 이를 파악  \n#merchandise - 이건 아이디당 하나라 count가 적용이 안됨. 이에, 그냥 아이디랑 올려주는데 나중에 레이블인코더 실시","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3/5 변수 추가\n#time - 구매와 구매 사이의 구간이 짧으면 로봇일 확률\n# 하나의 id당 최초구매 최근(마지막) 구매\n# 이를 id당 구매횟수로 나눠줌 \n\ntime_gap = bids.groupby('bidder_id')['time'].apply(lambda x : x.max() - x.min())\n\n\n# country nunique / device nunique들이 ip에 이미 포함됨. 이에 ip를 추가하면 과대적합 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\n# bids['time'].apply(lambda x : datetime.fromtimestamp(x/7000000))\n# year 311228 is out of range 천에서 십만으로 나눠줌\n# 5000년도를 그대로 해도 되지만\n# 7백만\n\nbids['datetime'] = bids['time'].apply(lambda x : datetime.fromtimestamp(x/7000000))\nbids.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bids['year'] = bids['datetime'].dt.year\nbids['month'] = bids['datetime'].dt.month\nbids['day'] = bids['datetime'].dt.day\nbids.head()\n\n# 하나의 id당 여러 타임들이 가져올 수 있어서 연도의평균 월의 평균이 필요\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_mean = bids.groupby('bidder_id')['year','month','day'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device_nunique = bids.groupby('bidder_id')['device'].nunique().reset_index()\ncountry_nunique = bids.groupby('bidder_id')['country'].nunique().reset_index()\nauction_nunique = bids.groupby('bidder_id')['auction'].nunique().reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nunique = bids.groupby('bidder_id')['auction','device','country'] # 여기서 미리 만들고 merge\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# alldata = pd.concat([train, test])\n# alldata = pd.merge(alldata,bid_count,on='bidder_id',how='left')\n# alldata.h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata = pd.concat([train, test])\nalldata = pd.merge(alldata,bid_count,on='bidder_id',how='left')\nalldata = pd.merge(alldata,auction_nunique,on='bidder_id',how='left')\nalldata = pd.merge(alldata,device_nunique,on='bidder_id',how='left')\nalldata = pd.merge(alldata,country_nunique,on='bidder_id',how='left')\nalldata = pd.merge(alldata,time_gap.reset_index(),on='bidder_id',how='left')\nalldata = pd.merge(alldata,date_mean,on='bidder_id',how='left') # 활동하는 패턴/시기 파악 \nalldata.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['time'] = alldata['time']/alldata['bid_id']  #교호작용\nalldata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata3 = alldata[alldata['bid_id']<20000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 그래프를 통해 bid_id가 유의미한 데이터인지 확인\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(20,12)) # 문자형(0, 1) / 숫자형(bid id) -> \n# sns.boxplot(alldata['outcome'],alldata['bid_id']) # 아웃라이어 떄문에 그래프가 이상함 -> 조건을 줘서 얘네 제외\nsns.boxplot(alldata3['outcome'],alldata3['bid_id']) # 대부분의 로봇은 구매횟수가 높구나","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 그래프를 통해 bid_id가 유의미한 데이터인지 확인\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(20,12)) # 문자형(0, 1) / 숫자형(bid id) -> \n# sns.boxplot(alldata['outcome'],alldata['bid_id']) # 아웃라이어 떄문에 그래프가 이상함 -> 조건을 줘서 얘네 제외\nsns.boxplot(alldata['outcome'],alldata['auction']) # 대부분의 로봇은 구매횟수가 높구나","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata4 = alldata[alldata['time']<5000000000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 그래프를 통해 bid_id가 유의미한 데이터인지 확인\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(20,12)) # 문자형(0, 1) / 숫자형(bid id) -> \n# sns.boxplot(alldata['outcome'],alldata['bid_id']) # 아웃라이어 떄문에 그래프가 이상함 -> 조건을 줘서 얘네 제외\nsns.boxplot(alldata['outcome'],alldata4['time']) # 인간과 로봇간의 차이가 확연한지 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata3 = alldata[alldata['auction']<20000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['payment_account'] = alldata['payment_account'].apply(lambda x:x[:5]) # ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['address'].sort_values(ascending = False).head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata['address'] = alldata['address'].apply(lambda x:x[:7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nalldata['payment_account'] = le.fit_transform(alldata['payment_account'])\nalldata['address'] = le.fit_transform(alldata['address'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import matplotlib.pyplot as plt\n#import seaborn as sns\n\n#plt.figure(figsize = (20,12))\n# sns.boxplot(train3['week'],train['Weekly_Sales'])\n\n#sns.distplot(np.log(train3['Weekly_Sales']))\n\n#plt.train['year']\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata2 = alldata.drop(['outcome', 'bidder_id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata2 = alldata2.fillna(-1) # 왠만하면 다른 데이터랑 안겹치게 넣어줌 -> -1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = alldata2[:len(train)]\ntest2 = alldata2[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# rf = RandomForestClassifier(n_jobs = 4)\n# rf.fit(train2, train['outcome'])\n# # result = rf.predict(test2)\n# result = rf.predict_proba(test2)\n\n# 3/5\n# 데이터 복잡도 파악 / 데이터를 모델에 최적합하게 적용 > 데이터의 특징 도출 \n# 데이터 복잡도 파악 방법\n# 1) 데이터의 갯수 (만개에서 몇만개면 어느정도 복잡 이때 depth =6이 적당, 여기선 2000개라 단순한편 = 행 갯수)\n# 2) 데이터의 칼럼 갯수 (10~30개면 중간정도 복잡도 30~ 100개면 복잡, 여기선 칼럼이 단순한편)\n# 3) 칼럼 중 카테고리형 파악하고 이 범주(종류의 갯수)가 몇개인지 = 트리 모델이 많은 질문 필요 = 복잡 ***\n# ** 4) y값 학습 시, 카테고리형 칼럼이 얼마나 중요하게 학습했는지 / 해당 칼럼이 중요치 않으면 아예 학습을 안하게됨 => 복잡하다고 느끼지 않음 -> 추가로 처리 필요\n# 항상 데이터셋이 복잡한지 안복잡한지 파악\n# 이 데이터셋은 단순한편 max_depth를 높게 해주면 바로 과대적합 문제가 일어남 -> 이에 max_depth를 일부러 낮게 맞춰줌, 7정도까지만 \n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_jobs = 4, max_depth=7, n_estimators=170) \n# max_depth 기본값이 -1, none \n# 하나의 나무에서 알 수 있는 정보가 줄어듬 예전에는 무한정이었음\n# 이에 depth (한 가지) 자체가 줄었기때문에 한 가지의 나무의 갯수(n_estimators)를 늘려줌 \nrf.fit(train2, train['outcome'])\n# result = rf.predict(test2)\nresult = rf.predict_proba(test2)\n\n# 데이터셋이 단순해 이에 맞춰 모델링 진행하니 결과가 좋아짐\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result #쉼표를 기준으로 왼쪽이 인간이 확률 = 0 / 오른쪽이 로봇일 확률 = 1 -> 대부분의 대회는 오른쪽에 있는 컬럼을 제출 \nresult = result[:,1]\n# 3번째가 0.38로 높음, 로봇일 확률이 큰거임 0.05 가 원래 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/facebook-recruiting-iv-human-or-bot/sampleSubmission.csv')\nsub['prediction'] = result\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('class4.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}